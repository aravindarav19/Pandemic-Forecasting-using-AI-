{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the data \n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\Optimized_Covid19_Dataset.csv\")\n",
    "df = df.sample(frac=0.2)  # Randomly sample 20% of the data for downsampling\n",
    "\n",
    "# Defining  the default feature vector (ensuring  all the  nodes have these features)\n",
    "default_feature = torch.tensor([0.0, 0.0, 0.0], dtype=torch.float)  # Default feature vector for nodes\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "df[['Population', 'Cases', 'Deaths']] = scaler.fit_transform(df[['Population', 'Cases', 'Deaths']])  # Standardizing  the features\n",
    "\n",
    "# Creating  a graph\n",
    "G = nx.Graph()  # Initialize graph\n",
    "\n",
    "# Adding nodes with features\n",
    "for i, row in df.iterrows():\n",
    "    features = torch.tensor([row['Population'], row['Cases'], row['Deaths']], dtype=torch.float)  # Extracting features\n",
    "    G.add_node(i, x=features)  # Adding  node with features\n",
    "\n",
    "# Grouping by state and creating edges more efficiently\n",
    "state_groups = df.groupby('Province_State').groups  # Grouping nodes by 'Province_State'\n",
    "\n",
    "for state, indices in state_groups.items():\n",
    "    edges = [(i, j) for idx, i in enumerate(indices) for j in indices[idx + 1:]]  # Create edges within the same state\n",
    "    G.add_edges_from(edges)  # Adding edges to the graph\n",
    "\n",
    "# Checking and ensuring all nodes have the same features\n",
    "for node in G.nodes:\n",
    "    if 'x' not in G.nodes[node]:\n",
    "        G.nodes[node]['x'] = default_feature  # Assiging  default features if missing\n",
    "\n",
    "# Converting to PyTorch Geometric data\n",
    "data = from_networkx(G)  # Converting NetworkX graph to PyTorch Geometric format\n",
    "\n",
    "# Defining a GCN model with more layers and dropout\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.4):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)  # First GCN layer\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)  # Second GCN layer\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)  # Third GCN layer (output)\n",
    "        self.dropout_rate = dropout_rate  # Dropout rate\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)  # Applying first GCN layer\n",
    "        x = F.relu(x)  # Applyng ReLU activation\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)  # dropout\n",
    "        x = self.conv2(x, edge_index)  # Appling second GCN layer\n",
    "        x = F.relu(x)  # Applying ReLU activation\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)  # Applying dropout\n",
    "        x = self.conv3(x, edge_index)  # Applying third GCN layer (output)\n",
    "        return x  # Return the final output\n",
    "\n",
    "model = GCN(input_dim=3, hidden_dim=38, output_dim=1, dropout_rate=0.2)  # Initialize the GCN model\n",
    "\n",
    "# predicting next week's cases\n",
    "df['Next_Week_Cases'] = df.groupby('Counties')['Cases'].shift(-1).fillna(0)  # Creating target variable for next week's cases\n",
    "y = torch.tensor(df['Next_Week_Cases'].values, dtype=torch.float).unsqueeze(1)  # Converting target to tensor\n",
    "\n",
    "# Spliting data into train and validation\n",
    "indices = list(range(len(df)))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)  # Split indices into train and validation sets\n",
    "\n",
    "# Creating masks\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True  # Mask for training nodes\n",
    "\n",
    "val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "val_mask[val_indices] = True  # Mask for validation nodes\n",
    "\n",
    "# Defining optimizer with L2 regularization (weight decay)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008, weight_decay=1e-4)  # Adam optimizer with L2 regularization\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(100):  # Training for up to 100 epochs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  \n",
    "    output = model(data) \n",
    "    train_output = output[train_mask]  # Getting the output for training data\n",
    "    loss = criterion(train_output, y[train_mask])  # Calculating training loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = output[val_mask]  # Get the output for validation data\n",
    "        val_loss = criterion(val_output, y[val_mask])  # Calculate validation loss\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}')  # Print training and validation loss\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss \n",
    "        patience_counter = 0 \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping due to no improvement in validation loss\")  # Early stopping condition\n",
    "        break\n",
    "\n",
    "# Evaluating with regression metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(data) \n",
    "    y_true = y.numpy().flatten()  # Ground truth values\n",
    "    y_pred = predictions.numpy().flatten()  # Predicted values\n",
    "\n",
    "    # Convert predictions to binary using a threshold for classification metrics\n",
    "    threshold = 0.5  # Define threshold for binary classification\n",
    "    y_binary = (y_true > threshold).astype(int)  \n",
    "    y_pred_binary = (y_pred > threshold).astype(int)  \n",
    "\n",
    "    # Calculating classification metrics\n",
    "    accuracy = accuracy_score(y_binary, y_pred_binary)  # Calculate accuracy\n",
    "    precision = precision_score(y_binary, y_pred_binary, average='macro')  # Calculate precision\n",
    "    recall = recall_score(y_binary, y_pred_binary, average='macro')  # Calculate recall\n",
    "    f1 = f1_score(y_binary, y_pred_binary, average='macro')  # Calculate F1-score\n",
    "    conf_matrix = confusion_matrix(y_binary, y_pred_binary)  # Generate confusion matrix\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))  # Root Mean Squared Error\n",
    "\n",
    "    # Calculate R-squared\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)  # Sum of squares of residuals\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)  # R-squared calculation\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision (Macro): {precision:.4f}')\n",
    "    print(f'Recall (Sensitivity, Macro): {recall:.4f}')\n",
    "    print(f'F1-Score (Macro): {f1:.4f}')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "    print(f'RMSE: {rmse:.4f}')  # Print RMSE\n",
    "    print(f'R-squared: {r2:.4f}')  # Print R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Loading the data \n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\Optimized_Covid19_Dataset.csv\")\n",
    "df = df.sample(frac=0.2)  # Randomly sample 20% of the data for downsampling\n",
    "\n",
    "# Preprocess the data\n",
    "county_data = df[df['Counties'] == df['Counties'].iloc[0]]  # Filter by the first county for example\n",
    "time_series_data = county_data[['Weeks', 'Cases', 'Deaths']].set_index('Weeks')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data)\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "seq_length = 4\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "# Reshape y for SMOTE\n",
    "threshold = 0.5  # Assuming binary classification for 'Deaths' > 0\n",
    "y_classification = (y[:, 1] > threshold).astype(int)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_classification, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(k_neighbors=2, random_state=42)\n",
    "X_train_res, y_train_class_res = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train_class)\n",
    "X_train_res = X_train_res.reshape(-1, seq_length, 2)\n",
    "\n",
    "\n",
    "# To match the new resampled labels, replicate y_train values based on y_train_class_res\n",
    "y_train_res = np.array([y_train[i % len(y_train)] for i in range(len(y_train_class_res))])\n",
    "\n",
    "# Define the LSTM model with regularization\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(seq_length, 2), return_sequences=True, kernel_regularizer=l2(0.001)),\n",
    "    LSTM(50, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(25, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dense(2)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_res, y_train_res, epochs=100, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Forecast future data\n",
    "predicted = model.predict(X_test)\n",
    "predicted = scaler.inverse_transform(predicted)\n",
    "\n",
    "# Convert predictions back to the original scale\n",
    "y_test_orig = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Filter out near-zero values\n",
    "near_zero_threshold = 1e-6\n",
    "filtered_indices = np.abs(y_test_orig[:, 1]) > near_zero_threshold  # Adjusting the threshold if needed\n",
    "y_test_filtered = y_test_orig[filtered_indices]\n",
    "predicted_filtered = predicted[filtered_indices]\n",
    "\n",
    "# For classification metrics, we'll need to threshold the predictions and actuals\n",
    "predicted_binary = (predicted[:, 1] > threshold).astype(int)  # For the 'Deaths' column\n",
    "y_test_binary = (y_test_orig[:, 1] > threshold).astype(int)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test_binary, predicted_binary)\n",
    "precision = precision_score(y_test_binary, predicted_binary, average='macro')\n",
    "recall = recall_score(y_test_binary, predicted_binary, average='macro')\n",
    "f1 = f1_score(y_test_binary, predicted_binary, average='macro')\n",
    "conf_matrix = confusion_matrix(y_test_binary, predicted_binary)\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision (Macro): {precision:.4f}')\n",
    "print(f'Recall (Sensitivity, Macro): {recall:.4f}')\n",
    "print(f'F1-Score (Macro): {f1:.4f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "# Print each metric explicitly\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision (Macro): {precision:.4f}')\n",
    "print(f'Recall (Sensitivity, Macro): {recall:.4f}')\n",
    "print(f'F1-Score (Macro): {f1:.4f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Compute RMSE for the 'Deaths' column (or the full data if you prefer)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_filtered[:, 1], predicted_filtered[:, 1]))\n",
    "print(f'RMSE (Deaths): {rmse:.4f}')\n",
    "\n",
    "# Compute R-squared for the 'Deaths' column\n",
    "r2 = r2_score(y_test_filtered[:, 1], predicted_filtered[:, 1])\n",
    "print(f'R-squared (Deaths): {r2:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
